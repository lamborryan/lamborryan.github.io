
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  Gobblin - 追风的蓝宝
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="爱好大数据与开源技术">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="追风的蓝宝" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html">追风的蓝宝</a></h1>
  
    <h2>爱好大数据与开源技术</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:www.lamborryan.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="_self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

  <li id=""><a target="_self" href="about.html">About</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div class="blog-index">

	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15155073098227.html">Gobblin系列(8)之Extractor源码分析</a></h1>
			<p class="meta"><time datetime="2018-01-09T22:15:09+08:00" 
			pubdate data-updated="true">2018/1/9</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<ul>
<li>
<a href="#toc_0">一. 简介</a>
</li>
<li>
<a href="#toc_1">二. MysqlExtractor的类继承关系。</a>
</li>
<li>
<a href="#toc_2">三. Extractor接口</a>
</li>
<li>
<a href="#toc_3">四: QueryBasedExtractor</a>
</li>
<li>
<a href="#toc_4">五: JdbcExtractor 和 MysqlExtractor</a>
</li>
<li>
<a href="#toc_5">六: 总结</a>
<ul>
<li>
<a href="#toc_6">本文完</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">一. 简介</h2>

<p>Extractor相比Source来说就简单多了, 因为Source不但要考虑当前job的WorkUnits, 还要考虑前一个Job的WorkUnits. 相比之下Extractor的服务对象就单个WorkUnits, 要实现的功能也无非就是根据low water mark和high water mark从数据源那里获取数据。</p>

<p>因此本文主要介绍Extractor如何实现上述描述的功能。目前Gobblin支持且常用的Extractor有MysqlExtractor, KafkaSimpleSource等等, 本文依然以MysqlExtractor为例来介绍Extractor。</p>

<!--more-->

<h2 id="toc_1">二. MysqlExtractor的类继承关系。</h2>

<p><img src="media/15155073098227/gobblin-extractor-1.png" alt="gobblin-extractor-1"/></p>

<p>由此可见, 要实现MysqlExtractor不但需要继承Extractor, 也要继承实现该数据源有关的SpecificLayer接口。</p>

<h2 id="toc_2">三. Extractor接口</h2>

<p>现在让我们来看下Extractor接口实现了哪些功能:</p>

<ul>
<li>获取record的数据结构</li>
<li>读取record</li>
<li>获取record的个数</li>
<li>获取可以获取到的high water mark</li>
</ul>

<pre><code class="language-java">public interface Extractor&lt;S, D&gt; extends Closeable {

  public S getSchema() throws IOException;

  public D readRecord(@Deprecated D reuse) throws DataRecordException, IOException;

  public long getExpectedRecordCount();

  @Deprecated
  public long getHighWatermark();
}
</code></pre>

<h2 id="toc_3">四: QueryBasedExtractor</h2>

<p>实际上QueryBasedExtractor已经通过ProtocolSpecificLayer的接口实现了Extractor的接口功能。</p>

<p>比如readRecord通过getIterator()来实现迭代器从而获取一条条record。其中getRecordSetFromSourceApi和getRecordSet就是ProtocolSpecificLayer的接口。如此后续要继承QueryBasedExtractor就必须实现ProtocolSpecificLayer的接口。</p>

<p>getRecordSetFromSourceApi和getRecordSet是通过SOURCE_QUERYBASED_IS_SPECIFIC_API_ACTIVE这个配置项来选择的。</p>

<pre><code class="language-java">/**
 * Get iterator from protocol specific api if is.specific.api.active is false
 * Get iterator from source specific api if is.specific.api.active is true
 * @return iterator
 */
private Iterator&lt;D&gt; getIterator()
    throws DataRecordException, IOException {
  if (Boolean.valueOf(this.workUnit.getProp(ConfigurationKeys.SOURCE_QUERYBASED_IS_SPECIFIC_API_ACTIVE))) {
    return this.getRecordSetFromSourceApi(this.schema, this.entity, this.workUnit, this.predicateList);
  }
  return this.getRecordSet(this.schema, this.entity, this.workUnit, this.predicateList);
}
</code></pre>

<p>ProtocolSpecificLayer的接口主要是获取数据源的record schema，record数量, data type, 以及获取record等。</p>

<pre><code class="language-java">public interface ProtocolSpecificLayer&lt;S, D&gt; {
  public void extractMetadata(String schema, String entity, WorkUnit workUnit)
      throws SchemaException, IOException;

  public long getMaxWatermark(String schema, String entity, String watermarkColumn,
      List&lt;Predicate&gt; snapshotPredicateList, String watermarkSourceFormat)
      throws HighWatermarkException;

  public long getSourceCount(String schema, String entity, WorkUnit workUnit, List&lt;Predicate&gt; predicateList)
      throws RecordCountException;

  public Iterator&lt;D&gt; getRecordSet(String schema, String entity, WorkUnit workUnit, List&lt;Predicate&gt; predicateList)
      throws DataRecordException, IOException;

  public String getWatermarkSourceFormat(WatermarkType watermarkType);
  ...

  public Map&lt;String, String&gt; getDataTypeMap();

  public Iterator&lt;D&gt; getRecordSetFromSourceApi(String schema, String entity, WorkUnit workUnit,
      List&lt;Predicate&gt; predicateList)
      throws IOException;
}
</code></pre>

<h2 id="toc_4">五: JdbcExtractor 和 MysqlExtractor</h2>

<p>JdbcExtractor的情况跟QueryBasedExtractor类似, 它继承了QueryBasedExtractor,SourceSpecificLayer,JdbcSpecificLayer, 通过JdbcSpecificLayer和SourceSpecificLayer的接口来实现QueryBasedExtractor中未实现的接口。</p>

<p>而且JdbcExtractor自动将jdbc的数据转换成了json格式。</p>

<p>MysqlExtractor又最后实现了剩余的接口。</p>

<h2 id="toc_5">六: 总结</h2>

<p>由于代码比较多, 但逻辑比较简单，所以对Extractor的介绍就简单掠过了。</p>

<h3 id="toc_6">本文完</h3>

<ul>
<li>原创文章，转载请注明： 转载自<a href="http://lamborryan.github.io">Lamborryan</a>，作者：<a href="http://lamborryan.github.io/about/">Ruan Chengfeng</a></li>
<li>本文链接地址：<a href="http://lamborryan.github.io/gobblin-extractor">http://lamborryan.github.io/gobblin-extractor</a></li>
<li>本文基于<a href="http://creativecommons.org/licenses/by/2.5/cn/">署名2.5中国大陆许可协议</a>发布，欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。 如您有任何疑问或者授权方面的协商，请邮件联系我。</li>
</ul>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15155071664887.html">Gobblin系列(7)之Source源码分析</a></h1>
			<p class="meta"><time datetime="2018-01-09T22:12:46+08:00" 
			pubdate data-updated="true">2018/1/9</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<ul>
<li>
<a href="#toc_0">一.简介</a>
</li>
<li>
<a href="#toc_1">三.AbstractSource</a>
</li>
<li>
<a href="#toc_2">四.QueryBasedSource</a>
<ul>
<li>
<a href="#toc_3">4.1. getLatestWatermarkFromMetadata</a>
</li>
<li>
<a href="#toc_4">4.2. Partitioner</a>
</li>
<li>
<a href="#toc_5">4.3. 总结</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">五.MysqlSource</a>
</li>
<li>
<a href="#toc_7">总结</a>
</li>
<li>
<a href="#toc_8">本文完</a>
</li>
</ul>


<h2 id="toc_0">一.简介</h2>

<p>Source在整个Gobblin任务流中负责以下三点:</p>

<ol>
<li>对数据源进行预切分获取<code>WorkUnits</code>。所谓预切分即是在不知道数据源是啥样的前提下, 划分好数据。</li>
<li>为每一个<code>task</code>生成一个<code>extractor</code>, 一般情况下一个<code>task</code>对应一个<code>WorkUnit</code>(当然也存在多对多的情况), 从而实现对数据的摄取。</li>
<li>提供<code>shutdown</code>处理机制, 即在job完成时候gobblin会回调<code>shutdown</code>函数, 我们可以在这里进行相应的逻辑处理。</li>
</ol>

<p>这些功能都是通过Source接口实现的。</p>

<!--more-->

<pre><code class="language-java">/*
 * An interface for classes that the end users implement to work with a data source from which
 * schema and data records can be extracted.
 * An implementation of this interface should contain all the logic required to work with a specific data source. This usually includes work determination and partitioning, and details of the connection protocol to work with the data source.
 */
public interface Source&lt;S, D&gt; {

  /**
   * Get a list of WorkUnits, each of which is for extracting a portion of the data.

   * Each WorkUnit will be used instantiate a WorkUnitState that gets passed to the
   * getExtractor(WorkUnitState) method to get an Extractor for extracting schema and data records
   * from the source. The WorkUnit instance should have all the properties needed for the Extractor
   * to work.

   * Typically the list of WorkUnits for the current run is determined by taking into account the
   * list of WorkUnits from the previous run so data gets extracted incrementally. The method  
   * SourceState.getPreviousWorkUnitStates can be used to get the list of WorkUnits from the
   * previous run.
   */
  public abstract List&lt;WorkUnit&gt; getWorkunits(SourceState state);

  /**
   * Get an Extractor based on a given WorkUnitState.

   * The Extractor returned can use WorkUnitState to store arbitrary key-value pairs that will be
   * persisted to the state store and loaded in the next scheduled job run.
   */
  public abstract Extractor&lt;S, D&gt; getExtractor(WorkUnitState state)
      throws IOException;

  /**
   * This method is called once when the job completes. Properties (key-value pairs) added to the
   * input SourceState instance will be persisted and available to the next scheduled job run
   * through the method getWorkunits(SourceState). If there is no cleanup or reporting required for
   * a particular implementation of this interface, then it is acceptable to have a default
   * implementation of this method.
   */
  public abstract void shutdown(SourceState state);
}
</code></pre>

<p>所有的数据源都是基于Source接口实现的, 通过继承并实现这三个方法我们就可以实现自己的Source了。</p>

<p>下图是 gobblin-0.6.2自带的Source的子类。</p>

<p><img src="media/15155071664887/gobblin-source-1.png" alt="gobblin-source-1"/></p>

<p>其中</p>

<ol>
<li><code>SimpleJsonSource</code>和<code>WikipediaSource</code>都是自带的Source example</li>
<li><code>SourceDecorator</code>是Source的适配器, Gobblin-Runtime对source的调用都是通过<code>SourceDecorator</code>实现的。</li>
<li>复杂点的Source都是从<code>AbstractSource</code>, <code>AbstractSource</code>有啥作用, 它加入了对上一个job的<code>WorkUnits</code>处理, 如果上一个job周期内有<code>WorkUnits</code>处理失败了，这些<code>WorkUnits</code>就会加入到本次<code>WorkUnits</code>中，具体过程待我下节慢慢讲来。</li>
</ol>

<p>目前使用较多的数据源主要是<code>MysqlSource</code>和<code>KafkaSimpleSource</code>。</p>

<p>本文主要以<code>AbstractSource</code>-&gt;<code>QueryBasedSource</code>－&gt;<code>MysqlSource</code>这一继承关系为例来介绍Gobblin是如何实现Source的。由于关于<code>Extractor</code>将会在下篇文章中单独介绍, 因此本文主要介绍Source的<code>getWorkunits()</code>。</p>

<h2 id="toc_1">三.AbstractSource</h2>

<p>Gobblin在获取WorkUnits时候不但会根据water marks来切分当前job的WorkUnits, 而且也会根据SourceState中获取上一个job的运行状态并根据策略配置选择是否要把上一个job的WorkUnits也加进去。(关于SourceState的介绍请看<a href="http://lamborryan.github.io/gobblin-state/">《Gobblin系列六之State》</a>)</p>

<p>因此AbstractSource就是在Source的基础上增加了获取上次job中需要在本次job重新运行的WorkUnits的方法。</p>

<blockquote>
<p>We use two keys for configuring work unit retries. The first one specifies whether work unit retries are enabled or not. This is for individual jobs or a group of jobs that following the same rule for work unit retries. The second one that is more advanced is for specifying a retry policy. This one is particularly useful for being a global policy for a group of jobs that have different job commit policies and want work unit retries only for a specific job commit policy. The first one probably is sufficient for most jobs that only need a way to enable/disable work unit retries. The second one gives users more flexibilities.</p>
</blockquote>

<p>关于WorkUnitRetryPolicy策略有以下几种:</p>

<ul>
<li><code>WorkUnitRetryPolicy.ALWAYS</code>: 总会对失败或者异常的work units进行retry, 不管采用了何种提交策略。</li>
<li><code>WorkUnitRetryPolicy.ONPARTIAL</code>: 只有当提交策略是COMMIT_ON_PARTIAL_SUCCESS时候才会失败或者异常的work units进行retry.
该选项往往用在对具有不同提交策略的job group上,做为全局的Retry策略。</li>
<li><code>WorkUnitRetryPolicy.ONFULL</code>: 只有当提交策略是COMMIT_ON_FULL_SUCCESS时候才会失败或者异常的work units进行retry.
该选项往往用在对具有不同提交策略的job group上,做为全局的Retry策略。</li>
<li><code>WorkUnitRetryPolicy.NEVER</code>: 从不对失败或者异常的work units进行retry.</li>
</ul>

<pre><code class="language-java">protected List&lt;WorkUnitState&gt; getPreviousWorkUnitStatesForRetry(SourceState state) {
  * * *
  // 获取retry策略
  WorkUnitRetryPolicy workUnitRetryPolicy;
  if (state.contains(ConfigurationKeys.WORK_UNIT_RETRY_POLICY_KEY)) {
    // Use the given work unit retry policy if specified
    workUnitRetryPolicy = WorkUnitRetryPolicy.forName(state.getProp(ConfigurationKeys.WORK_UNIT_RETRY_POLICY_KEY));
  } else {
    // 根据WORK_UNIT_RETRY_ENABLED_KEY这个配置来决定是否打开WorkUnitRetryPolicy策略
    boolean retryFailedWorkUnits = state.getPropAsBoolean(ConfigurationKeys.WORK_UNIT_RETRY_ENABLED_KEY, true);
    workUnitRetryPolicy = retryFailedWorkUnits ? WorkUnitRetryPolicy.ALWAYS : WorkUnitRetryPolicy.NEVER;
  }

  // 如果是never策略则返回空的workunit
  if (workUnitRetryPolicy == WorkUnitRetryPolicy.NEVER) {
    return ImmutableList.of();
  }

  List&lt;WorkUnitState&gt; previousWorkUnitStates = Lists.newArrayList();
  // 获取上一个job的没有成功的workunit。
  for (WorkUnitState workUnitState : state.getPreviousWorkUnitStates()) {
    if (workUnitState.getWorkingState() != WorkUnitState.WorkingState.COMMITTED) {
      if (state.getPropAsBoolean(ConfigurationKeys.OVERWRITE_CONFIGS_IN_STATESTORE,
          ConfigurationKeys.DEFAULT_OVERWRITE_CONFIGS_IN_STATESTORE)) {
        // We need to make a copy here since getPreviousWorkUnitStates returns ImmutableWorkUnitStates
        // for which addAll is not supported
        WorkUnitState workUnitStateCopy = new WorkUnitState(workUnitState.getWorkunit());
        workUnitStateCopy.addAll(workUnitState);
        workUnitStateCopy.overrideWith(state);
        previousWorkUnitStates.add(workUnitStateCopy);
      } else {
        previousWorkUnitStates.add(workUnitState);
      }
    }
  }

  // 如果是always策略, 则直接返回上一个job所有失败的workunits
  if (workUnitRetryPolicy == WorkUnitRetryPolicy.ALWAYS) {
    return previousWorkUnitStates;
  }

  // 获取提交策略，默认是全部提交。
  JobCommitPolicy jobCommitPolicy = JobCommitPolicy
      .forName(state.getProp(ConfigurationKeys.JOB_COMMIT_POLICY_KEY, ConfigurationKeys.DEFAULT_JOB_COMMIT_POLICY));

  // 根据提交策略和retry策略来决定是否需要retry上一个job失败的workunits
  if ((workUnitRetryPolicy == WorkUnitRetryPolicy.ON_COMMIT_ON_PARTIAL_SUCCESS
      &amp;&amp; jobCommitPolicy == JobCommitPolicy.COMMIT_ON_PARTIAL_SUCCESS)
      || (workUnitRetryPolicy == WorkUnitRetryPolicy.ON_COMMIT_ON_FULL_SUCCESS
          &amp;&amp; jobCommitPolicy == JobCommitPolicy.COMMIT_ON_FULL_SUCCESS)) {
    return previousWorkUnitStates;
  } else {
    // Return an empty list if job commit policy and work unit retry policy do not match
    return ImmutableList.of();
  }
}
</code></pre>

<p>由代码可以看出, 对于上一个失败的workunits, 要么全部retry, 要么都不retry, 在颗粒度比较粗。</p>

<p>同时AbstractSource还会生成Extract State. 相同的namespace和table具有相同的Extract。比如kafka sourc, 虽然只是一次job, 但是有可能因为存在多个topic从而产生了不同的Extract, 即每一个top的Extract id不同。而Extract不同的一个好处是可以使用不同的发布策略。</p>

<pre><code class="language-java">public Extract createExtract(TableType type, String namespace, String table) {
    return this.extractFactory.getUniqueExtract(type, namespace, table);
}
</code></pre>

<h2 id="toc_2">四.QueryBasedSource</h2>

<p>QueryBasedSource在AbstractSource基础上又实现了query－based的getWorkunits， 已经具有很鲜明的sql特点了。<br/>
本小节主要介绍QueryBasedSource如何通过getWorkunits来获取WorkUnits.</p>

<p>QueryBasedSource的getWorkunits需要解决以下几个问题:</p>

<ul>
<li>怎么获取上一个job的Latest Water mark, 从而可以做为本次job的low Water mark</li>
<li>怎么进行partition</li>
</ul>

<p>因此分为两小节来分别介绍。</p>

<h3 id="toc_3">4.1. getLatestWatermarkFromMetadata</h3>

<ul>
<li>如果commit policy是full且有task失败了, 则Latest Water mark是所有WorkUnits中最小的low water mark。</li>
<li>如果commit policy是full且所有task成功, 则Latest Water mark是所有WorkUnits中最大的high water mark。</li>
<li>如果commit policy不是full且有task成功, 则Latest Water mark是所有成功的WorkUnits中最大的high water mark。失败的task由retry policy控制。</li>
<li>如果commit policy不是full且所有task都失败, 则Latest Water mark是所有WorkUnits中最小的low water mark。</li>
</ul>

<blockquote>
<p>这里我有疑惑: 如果设置了retry policy = JobCommitPolicy.COMMIT_ON_FULL_SUCCESS, retry policy会把上一个job的failed workunit加入到新job的workunits。 而getLatestWatermarkFromMetadata又会计算上一个job的最小的watermark，从而再次计算这些workunit。使得上一job的workunits重新跑两次。</p>
</blockquote>

<pre><code class="language-java">private long getLatestWatermarkFromMetadata(SourceState state) {
   ...
   boolean hasFailedRun = false;
   boolean isCommitOnFullSuccess = false;
   boolean isDataProcessedInPreviousRun = false;

   JobCommitPolicy commitPolicy = JobCommitPolicy
       .forName(state.getProp(ConfigurationKeys.JOB_COMMIT_POLICY_KEY, ConfigurationKeys.DEFAULT_JOB_COMMIT_POLICY));
   if (commitPolicy == JobCommitPolicy.COMMIT_ON_FULL_SUCCESS) {
     isCommitOnFullSuccess = true;
   }

   for (WorkUnitState workUnitState : previousWorkUnitStates) {
     long processedRecordCount = 0;
     if (workUnitState.getWorkingState() == WorkingState.FAILED
         || workUnitState.getWorkingState() == WorkingState.CANCELLED
         || workUnitState.getWorkingState() == WorkingState.RUNNING
         || workUnitState.getWorkingState() == WorkingState.PENDING) {
       hasFailedRun = true;
     } else {
       processedRecordCount = workUnitState.getPropAsLong(ConfigurationKeys.EXTRACTOR_ROWS_EXPECTED);
       if (processedRecordCount != 0) {
         isDataProcessedInPreviousRun = true;
       }
     }

     // Consider high water mark of the previous work unit, if it is
     // extracted any data
     if (processedRecordCount != 0) {
       previousWorkUnitStateHighWatermarks.add(workUnitState.getHighWaterMark());
     }

     previousWorkUnitLowWatermarks.add(this.getLowWatermarkFromWorkUnit(workUnitState));
   }

   // If commit policy is full and it has failed run, get latest water mark
   // as
   // minimum of low water marks from previous states.
   if (isCommitOnFullSuccess &amp;&amp; hasFailedRun) {
     long previousLowWatermark = Collections.min(previousWorkUnitLowWatermarks);

     WorkUnitState previousState = previousWorkUnitStates.get(0);
     ExtractType extractType =
         ExtractType.valueOf(previousState.getProp(ConfigurationKeys.SOURCE_QUERYBASED_EXTRACT_TYPE).toUpperCase());

     // add backup seconds only for snapshot extracts but not for appends
     if (extractType == ExtractType.SNAPSHOT) {
       int backupSecs = previousState.getPropAsInt(ConfigurationKeys.SOURCE_QUERYBASED_LOW_WATERMARK_BACKUP_SECS, 0);
       String watermarkType = previousState.getProp(ConfigurationKeys.SOURCE_QUERYBASED_WATERMARK_TYPE);
       latestWaterMark = this.addBackedUpSeconds(previousLowWatermark, backupSecs, watermarkType);
     } else {
       latestWaterMark = previousLowWatermark;
     }
   }

   // If commit policy is full and there are no failed tasks or commit
   // policy is partial,
   // get latest water mark as maximum of high water marks from previous
   // tasks.
   else {
     if (isDataProcessedInPreviousRun) {
       latestWaterMark = Collections.max(previousWorkUnitStateHighWatermarks);
     } else {
       latestWaterMark = Collections.min(previousWorkUnitLowWatermarks);
     }
   }

   return latestWaterMark;
 }
</code></pre>

<h3 id="toc_4">4.2. Partitioner</h3>

<p>Partitioner 的作用就是根据latestWaterMark来对数据进行partition, 并得到WorkUnits. Partition过程涉及到以下几个变量:</p>

<ul>
<li>interval, partition时候的最小单位, 由source.querybased.partition.interval获取。</li>
<li>maxPartitions, partition的最大个数, 由配置source.max.number.of.partitions获取。</li>
<li>lowWatermark, 根据previousWatermark(latestWaterMark)获取partition的左边界。</li>
<li>highWatermark, 计算partition的右边界。</li>
<li>source.querybased.watermark.type决定了是以哪种类型进行partition。</li>
</ul>

<p>如果lowWatermark或者highWatermark等于DEFAULT_WATERMARK_VALUE, 则只会形成一个partition。</p>

<pre><code class="language-java">public HashMap&lt;Long, Long&gt; getPartitions(long previousWatermark) {
   HashMap&lt;Long, Long&gt; defaultPartition = new HashMap&lt;Long, Long&gt;();
   ...
   // extract type 比如snapshot等
   ExtractType extractType =
       ExtractType.valueOf(this.state.getProp(ConfigurationKeys.SOURCE_QUERYBASED_EXTRACT_TYPE).toUpperCase());
    // watermarkType 类型 比如 timestamp date hour simple
   WatermarkType watermarkType =
       WatermarkType.valueOf(this.state.getProp(ConfigurationKeys.SOURCE_QUERYBASED_WATERMARK_TYPE,
           ConfigurationKeys.DEFAULT_WATERMARK_TYPE).toUpperCase());
   // 分区步长,最小单位
   int interval =
       this.getUpdatedInterval(this.state.getPropAsInt(ConfigurationKeys.SOURCE_QUERYBASED_PARTITION_INTERVAL, 0),
           extractType, watermarkType);
   int sourceMaxAllowedPartitions = this.state.getPropAsInt(ConfigurationKeys.SOURCE_MAX_NUMBER_OF_PARTITIONS, 0);
   // 最大可以分区的个数
   int maxPartitions =
       (sourceMaxAllowedPartitions != 0 ? sourceMaxAllowedPartitions
           : ConfigurationKeys.DEFAULT_MAX_NUMBER_OF_PARTITIONS);

   WatermarkPredicate watermark = new WatermarkPredicate(null, watermarkType);
   int deltaForNextWatermark = watermark.getDeltaNumForNextWatermark();

   // 可以分区的最小watermark
   long lowWatermark = this.getLowWatermark(extractType, watermarkType, previousWatermark, deltaForNextWatermark);
   // 可以分区的最大watermark
   long highWatermark = this.getHighWatermark(extractType, watermarkType);
   // 如果最小watermark或者最大watermark为－1, 则只有一个分区
   if (lowWatermark == ConfigurationKeys.DEFAULT_WATERMARK_VALUE
       || highWatermark == ConfigurationKeys.DEFAULT_WATERMARK_VALUE) {
     defaultPartition.put(lowWatermark, highWatermark);
     return defaultPartition;
   }
   // 根据相应的watertype进行分区, 实际上调用的是watermark的getIntervals
   return watermark.getPartitions(lowWatermark, highWatermark, interval, maxPartitions);
 }

</code></pre>

<p>这里要介绍下source.querybased.watermark.type这个配置, 它决定了是以哪种类型来进行partition。默认支持TIMESTAMP, DATE, HOUR, SIMPLE. 所谓的SIMPLE, 其实就是整数, Gobblin会根据这个type来决定哪种watermark。</p>

<pre><code class="language-java">public WatermarkPredicate(String watermarkColumn, WatermarkType watermarkType) {
    super();
    this.watermarkColumn = watermarkColumn;
    this.watermarkType = watermarkType;

    switch (watermarkType) {
    case TIMESTAMP:
      this.watermark = new TimestampWatermark(watermarkColumn, DEFAULT_WATERMARK_VALUE_FORMAT);
      break;
    case DATE:
      this.watermark = new DateWatermark(watermarkColumn, DEFAULT_WATERMARK_VALUE_FORMAT);
      break;
    case HOUR:
      this.watermark = new HourWatermark(watermarkColumn, DEFAULT_WATERMARK_VALUE_FORMAT);
      break;
    case SIMPLE:
      this.watermark = new SimpleWatermark(watermarkColumn, DEFAULT_WATERMARK_VALUE_FORMAT);
      break;
    default:
      this.watermark = new SimpleWatermark(watermarkColumn, DEFAULT_WATERMARK_VALUE_FORMAT);
      break;
    }
}
</code></pre>

<p>假设我们选择了TIMESTAMP, 则gobblin会实例化TimestampWatermark来进行partition。</p>

<pre><code class="language-java">synchronized public HashMap&lt;Long, Long&gt; getIntervals(long lowWatermarkValue, long highWatermarkValue, long partitionInterval, int maxIntervals) {
    ...
    HashMap&lt;Long, Long&gt; intervalMap = new HashMap&lt;Long, Long&gt;();
    Date startTime = new Date(lowWatermark);
    Date endTime = new Date(highWatermark);
    LOG.debug(&quot;Sart time:&quot; + startTime + &quot;; End time:&quot; + endTime);
    long lwm;
    long hwm;
    while (startTime.getTime() &lt;= endTime.getTime()) {
      lwm = Long.parseLong(INPUTFORMATPARSER.format(startTime));
      calendar.setTime(startTime);
      calendar.add(Calendar.HOUR, (int) interval);
      nextTime = calendar.getTime();
      hwm = Long.parseLong(INPUTFORMATPARSER.format(nextTime.getTime() &lt;= endTime.getTime() ? nextTime : endTime));
      intervalMap.put(lwm, hwm);
      LOG.debug(&quot;Partition - low:&quot; + lwm + &quot;; high:&quot; + hwm);
      calendar.add(Calendar.SECOND, deltaForNextWatermark);
      startTime = calendar.getTime();
    }

    return intervalMap;
}

</code></pre>

<p>需要说明一点就是, 如果watertype是timestamp,date,hour的highwatermark是当前时间, 如果是simple且snapshot的话, highwatermark是－1, 也就是说如果是simple的话只会有一个partition。</p>

<h3 id="toc_5">4.3. 总结</h3>

<p>最后回顾下getWorkunits的过程</p>

<pre><code class="language-java">public List&lt;WorkUnit&gt; getWorkunits(SourceState state) {
    ...
    // 获取上一个job的latest watermark
    long previousWatermark = this.getLatestWatermarkFromMetadata(state);

    // 根据latest watermark进行partition,并作排序
    Map&lt;Long, Long&gt; sortedPartitions = Maps.newTreeMap();
    sortedPartitions.putAll(new Partitioner(state).getPartitions(previousWatermark));

    // Use extract table name to create extract
    SourceState partitionState = new SourceState();
    partitionState.addAll(state);
    Extract extract = partitionState.createExtract(tableType, nameSpaceName, extractTableName);

    // Setting current time for the full extract
    if (Boolean.valueOf(state.getProp(ConfigurationKeys.EXTRACT_IS_FULL_KEY))) {
      extract.setFullTrue(System.currentTimeMillis());
    }

    // 将各paritions的watermark写入到workunits
    for (Entry&lt;Long, Long&gt; entry : sortedPartitions.entrySet()) {
      partitionState.setProp(ConfigurationKeys.WORK_UNIT_LOW_WATER_MARK_KEY, entry.getKey());
      partitionState.setProp(ConfigurationKeys.WORK_UNIT_HIGH_WATER_MARK_KEY, entry.getValue());
      workUnits.add(partitionState.createWorkUnit(extract));
    }

    // 将上一个job失败需要retry的workunit添加到本job的workunit中
    List&lt;WorkUnit&gt; previousWorkUnits = this.getPreviousWorkUnitsForRetry(state);
    LOG.info(&quot;Total number of incomplete tasks from the previous run: &quot; + previousWorkUnits.size());
    workUnits.addAll(previousWorkUnits);

    return workUnits;
  }
</code></pre>

<h2 id="toc_6">五.MysqlSource</h2>

<p>最后MysqlSource只实现了getExtractor接口，即返回一个getExtractor对象.</p>

<pre><code class="language-java">public class MysqlSource extends QueryBasedSource&lt;JsonArray, JsonElement&gt; {
    private static final Logger LOG = LoggerFactory.getLogger(MysqlSource.class);

    public Extractor&lt;JsonArray, JsonElement&gt; getExtractor(WorkUnitState state)
      throws IOException {
        Extractor&lt;JsonArray, JsonElement&gt; extractor = null;
        try {
          extractor = new MysqlExtractor(state).build();
        } catch (ExtractPrepareException e) {
          LOG.error(&quot;Failed to prepare extractor: error - &quot; + e.getMessage());
          throw new IOException(e);
        }
        return extractor;
    }
}
</code></pre>

<h2 id="toc_7">总结</h2>

<p>本文主要介绍了Source的几个重要功能, 以MysqlSource为例介绍了Source是如何getWorkunits的, 在这过程中又结合watermark简单描述了gobblin的retry策略。</p>

<h2 id="toc_8">本文完</h2>

<ul>
<li>原创文章，转载请注明： 转载自<a href="http://lamborryan.github.io">Lamborryan</a>，作者：<a href="http://lamborryan.github.io/about/">Ruan Chengfeng</a></li>
<li>本文链接地址：<a href="http://lamborryan.github.io/gobblin-source">http://lamborryan.github.io/gobblin-source</a></li>
<li>本文基于<a href="http://creativecommons.org/licenses/by/2.5/cn/">署名2.5中国大陆许可协议</a>发布，欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。 如您有任何疑问或者授权方面的协商，请邮件联系我。</li>
</ul>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15155068855798.html">Gobblin系列(6)之State</a></h1>
			<p class="meta"><time datetime="2018-01-09T22:08:05+08:00" 
			pubdate data-updated="true">2018/1/9</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<ul>
<li>
<a href="#toc_0">一. 简介</a>
</li>
<li>
<a href="#toc_1">二. SourceState, JobState, DatasetState</a>
<ul>
<li>
<a href="#toc_2">2.1.SourceState</a>
</li>
<li>
<a href="#toc_3">2.2.JobState</a>
</li>
<li>
<a href="#toc_4">2.3.DatasetState</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">三. WorkUnit, MutliWorkUnit</a>
<ul>
<li>
<a href="#toc_6">3.1.WorkUnit</a>
</li>
<li>
<a href="#toc_7">3.2.MutliWorkUnit</a>
</li>
</ul>
</li>
<li>
<a href="#toc_8">四. WorkUnitState, TaskState</a>
<ul>
<li>
<a href="#toc_9">4.1.WorkUnitState</a>
</li>
<li>
<a href="#toc_10">4.2.TaskState</a>
</li>
</ul>
</li>
<li>
<a href="#toc_11">五. Extract</a>
</li>
<li>
<a href="#toc_12">六. 运行过程转换</a>
</li>
<li>
<a href="#toc_13">七. 总结</a>
</li>
</ul>


<h2 id="toc_0">一. 简介</h2>

<p>Gobblin实在是有太多的state了, 比如SourceState, JobState, DatasetState, WorkUnit等等. 而这些State又跟整个Gobblin的各个阶段密切联系, 所以我独立出一篇文章来理顺下这些State。</p>

<!-- more-->

<p>本文借鉴了这篇文档<a href="http://gobblin.readthedocs.org/en/latest/user-guide/State-Management-and-Watermarks/#gobblin-state-deep-dive">《gobblin-state-deep-dive》</a></p>

<p>下图是Gobblin的类关系, 通过它我们可以对整个State体系有个初步了解:</p>

<p><img src="media/15155068855798/gobblin-state-1.png" alt="gobblin-state-1"/></p>

<p>基类<code>State</code>是<code>Properties</code>的封装, 并提供了对它的一系列方法。</p>

<h2 id="toc_1">二. SourceState, JobState, DatasetState</h2>

<h3 id="toc_2">2.1.SourceState</h3>

<p>SourceState主要有几个任务:</p>

<ol>
<li>包含当前job的配置文件的配置</li>
<li>包含上一个job在StateStore存储下来的State</li>
<li>为Source提供根据Extract生成WorkUnit的方法</li>
<li>提供生成Extract的方法</li>
</ol>

<p>因此SourceState主要作用阶段在source.</p>

<pre><code class="language-java">public class SourceState extends State {
    * * *
    /* 上一个job的state */
    private final Map&lt;String, SourceState&gt; previousDatasetStatesByUrns;
    private final List&lt;WorkUnitState&gt; previousWorkUnitStates = Lists.newArrayList();

    /* 创建Extract Statte */
    public synchronized Extract createExtract(Extract.TableType type, String namespace, String table) {
      Extract extract = new Extract(this, type, namespace, table);
      while (EXTRACT_SET.contains(extract)) {
        if (Strings.isNullOrEmpty(extract.getExtractId())) {
          extract.setExtractId(DTF.print(new DateTime()));
        } else {
          DateTime extractDateTime = DTF.parseDateTime(extract.getExtractId());
          extract.setExtractId(DTF.print(extractDateTime.plusSeconds(1)));
        }
      }
      EXTRACT_SET.add(extract);
      return extract;
    }

    /* 根据State创建WorkUnit Statte */
    public WorkUnit createWorkUnit(Extract extract) {
      return new WorkUnit(this, extract);
    }

    * * *
}
</code></pre>

<h3 id="toc_3">2.2.JobState</h3>

<p>JobState在SourceState基础上更进了一步, 它不但包含了SourceState的配置和功能, 更加入了job运行时的配置参数, 比如job ID, 开始时间, 结束时间, 以及job的运行状态等, 以及每个task的TaskState.</p>

<pre><code class="language-java">
public class JobState extends SourceState {

  /**
   * Job运行状态
   */
  public enum RunningState {
    PENDING, RUNNING, SUCCESSFUL, COMMITTED, FAILED, CANCELLED
  }

  private String jobName;   // job运行名字
  private String jobId;     // job id
  private long startTime = 0; // job开始时间
  private long endTime = 0; // job结束时间
  private long duration = 0; // 持续时间
  private RunningState state = RunningState.PENDING; //Job运行状态
  private int taskCount = 0; // task个数
  private final Map&lt;String, TaskState&gt; taskStates = Maps.newLinkedHashMap();// 所有task状态

}
</code></pre>

<p>因为JobState基本上包含了所有job的运行信息, 所以它的生命周期就是完整的job的生命周期, 特别是在metrics输出信息。</p>

<h3 id="toc_4">2.3.DatasetState</h3>

<p>DatasetState在JobState的基础上添加了dataset.urn属性以此来区分不同的dataset。目前主要用在<code>FsDatasetStateStore</code></p>

<h2 id="toc_5">三. WorkUnit, MutliWorkUnit</h2>

<h3 id="toc_6">3.1.WorkUnit</h3>

<p>WorkUnit主要包含以下几个内容:</p>

<ol>
<li>包含State即SourceState的所有properties, 这是因为WorkUnit是由Source的<code>getWorkunits(SourceState)</code>生成的</li>
<li>包含low watermark和high watermark的数据, 即摄取的数据的摄取范围</li>
<li>包含Extractor，摄取器.</li>
</ol>

<pre><code class="language-java">
public WorkUnit(SourceState state, Extract extract) {
  // Values should only be null for deserialization
  if (state != null) {
    super.addAll(state);
  }

  if (extract != null) {
    this.extract = extract;
  } else {
    this.extract = new Extract(null, null, null, null);
  }
}

public void setLowWaterMark(long lowWaterMark) {
  setProp(ConfigurationKeys.WORK_UNIT_LOW_WATER_MARK_KEY, lowWaterMark);
}

public void setHighWaterMark(long highWaterMark) {
  setProp(ConfigurationKeys.WORK_UNIT_HIGH_WATER_MARK_KEY, highWaterMark);
}
</code></pre>

<p>WorkUnit由Source的getWorkunits生成, 主要是为了记录不同task应该摄取哪部分数据.</p>

<h3 id="toc_7">3.2.MutliWorkUnit</h3>

<p>MutliWorkUnit继承了WorkUnit, 其实质则是对多个WorkUnit进行了封装以便后续运行在一个task中, 如果MutliWorkUnit包含了所有的WorkUnits, 那么一个job就只会对应一个task。</p>

<pre><code class="language-java">public class MultiWorkUnit extends WorkUnit {

  private final List&lt;WorkUnit&gt; workUnits = Lists.newArrayList();

  @Deprecated
  public MultiWorkUnit() {
    super();
  }

  * * *
}
</code></pre>

<p>MutliWorkUnit的出现很好对source的partition进行了补充, 使得它在负载均衡上面得到了很好的提升。如果没有MutliWorkUnit, 那么mapreduce模式下由于partition的不均衡很容易造成数据倾斜. 而MutliWorkUnit的存在使得我们可以通过合并小的WorkUnit的平衡每个map的数据, 降低数据倾斜的风险。</p>

<h2 id="toc_8">四. WorkUnitState, TaskState</h2>

<h3 id="toc_9">4.1.WorkUnitState</h3>

<p>WorkUnitState不但包含了WorkUnit的所有配置, 也包含了WorkUnit的运行状态以及非常重要的actual high watermark。</p>

<pre><code class="language-java">public class WorkUnitState extends State {
    public enum WorkingState {
    PENDING,
    RUNNING,
    SUCCESSFUL,
    COMMITTED,
    FAILED,
    CANCELLED
    }
    private WorkUnit workunit;
    public WorkUnitState(WorkUnit workUnit) {
        this.workunit = workUnit;
    }
    public void setActualHighWatermark(Watermark watermark) {
        setProp(ConfigurationKeys.WORK_UNIT_STATE_ACTUAL_HIGH_WATER_MARK_KEY, watermark.toJson().toString());
    }
}
</code></pre>

<blockquote>
<p>setActualHighWatermark 是在Extractor中初始化时候设置的，它必须赶在readRecord调用前进行set。</p>
</blockquote>

<h3 id="toc_10">4.2.TaskState</h3>

<p>TaskState在WorkUnitState的基础上加入了task有关的一些信息, 比如task id, task name, start time, end time 等。生命周期贯穿了整个task，类似于job task, 不同之处前者在task level，后者在task level。因此TaskState也会在metrix存储相应的运行信息。</p>

<pre><code class="language-java">
public class TaskState extends WorkUnitState {
    * * *
    private String jobId;
    private String taskId;
    private long startTime = 0;
    private long endTime = 0;
    private long duration;

    public TaskState(WorkUnitState workUnitState) {
        // Since getWorkunit() returns an immutable WorkUnit object,
        // the WorkUnit object in this object is also immutable.
        super(workUnitState.getWorkunit());
        addAll(workUnitState);
        this.jobId = workUnitState.getProp(ConfigurationKeys.JOB_ID_KEY);
        this.taskId = workUnitState.getProp(ConfigurationKeys.TASK_ID_KEY);
        this.setId(this.taskId);
    }

    * * *
}

</code></pre>

<h2 id="toc_11">五. Extract</h2>

<p>Extract对接需要摄入的数据源, 它包含了需要摄入的配置信息, 比如摄入方式(snapshot-only, append-only, snapshot-append), 主键(primary keys), 需要增量的键(delta fields)等等。</p>

<p>相同namespace和table下的Extracts必须有不同的extract ID。 一个或多个WorkUnits可以分享同一个extract ID。 具有相同的extract ID的WorkUnits可以视为同一个Extract的组成部分, 以便使用相同的发布策略。</p>

<h2 id="toc_12">六. 运行过程转换</h2>

<p>本小节主要介绍这些state在gobblin运行过程中的关系:</p>

<ul>
<li>当job开始运行时候, job launch首先会创建JobState, 它包含两部分信息, 1) 所有job的配置文件; 2) 上一个job的JobState / DatasetState, 以及其他的配置, 如每个JobState／DatasetState的 actual high watermark等</li>
<li>Job Launcher 会把JobState(以SourceState的形势)传给Souce, Source根据JobState来创建WorkUnits。当创建WorkUnits时候, JobState不会立马就加载进WorkUnits, 而是在task执行时候才会完成加载。这是因为当laucher运行在单个JVM时候,如果创建大量的WorkUnits且都是JobState copy有可能会出现oom。</li>
<li>job launcher 准备开始运行WorkUnits。</li>
<li>在standalone模式中, job launcher会把JobState添加到每一个WorkUnit, 如果配置已经在WorkUnit中则不会覆盖。对于每一个WorkUnit， job launcher都会创建一个task去运行WorkUnit，并提交所有task到TaskExecutor运行在同一个线程池中。</li>
<li>在MR模式下, job laucher会序列化JobState和每一个WorkUnit, 并被mapper所获取。</li>
</ul>

<p>此时 job launcher 会等待所有task完成。</p>

<ul>
<li>每个task对应一个workunit, 每个task包含一个TaskState。初始化时候，TaskState获取JobState以及workunit所有配置, 运行过程中, 更多的Extractor, Converter 和 Writer的配置, 比如actual high watermark, 会通过Extractor加入到TaskState。</li>
<li>当所有Task完成时候, TaskState会根据dataset.urn创建DatasetStates。当dataset的数据提交时候, job launcher就会存下DatasetState。 如果没有dataset.urn, 那么只会有单个DatasetState，并根据提交策略进行提交。</li>
</ul>

<h2 id="toc_13">七. 总结</h2>

<p>最后说下我的理解:</p>

<ul>
<li>WorkUnitState包含了WorkUnit的配置以及task运行的状态数据.</li>
<li>TaskState包含了WorkUnitState和Task的基本信息(id,start time, end time)</li>
<li>JobState包含配置文件配置, 所有task的TaskState，以及job的基础信息(id, start time, end time)。</li>
<li>在初始阶段, WorkUnits由JobState生成。</li>
</ul>

<p>本文完</p>

<ul>
<li>原创文章，转载请注明： 转载自<a href="http://lamborryan.github.io">Lamborryan</a>，作者：<a href="http://lamborryan.github.io/about/">Ruan Chengfeng</a></li>
<li>本文链接地址：<a href="http://lamborryan.github.io/gobblin-state">http://lamborryan.github.io/gobblin-state</a></li>
<li>本文基于<a href="http://creativecommons.org/licenses/by/2.5/cn/">署名2.5中国大陆许可协议</a>发布，欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。 如您有任何疑问或者授权方面的协商，请邮件联系我。</li>
</ul>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15155067914960.html">Gobblin系列(5)之Writer源码分析</a></h1>
			<p class="meta"><time datetime="2018-01-09T22:06:31+08:00" 
			pubdate data-updated="true">2018/1/9</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<ul>
<li>
<a href="#toc_0">简介</a>
</li>
<li>
<a href="#toc_1">流程简述</a>
</li>
<li>
<a href="#toc_2">PartitionedDataWriter</a>
<ul>
<li>
<ul>
<li>
<a href="#toc_3">PartitionedDataWriter的初始化</a>
</li>
<li>
<a href="#toc_4">PartitionedDataWriter的基类</a>
</li>
<li>
<a href="#toc_5">PartitionedDataWriter的具体实现</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_6">partition</a>
</li>
<li>
<a href="#toc_7">DataWriter</a>
</li>
<li>
<a href="#toc_8">Publiser</a>
</li>
<li>
<a href="#toc_9">总结</a>
</li>
</ul>


<h2 id="toc_0">简介</h2>

<p>Gobblin的writer功能还是很强大的, 该Stage负责将record写入临时文件中。由于项目的需要, 要求把每天的kafka日志按照日期按天输出到不同的目录。所以花了一天时间研究了Writer的源码和流程。</p>

<!--more-->

<h2 id="toc_1">流程简述</h2>

<p><img src="media/15155067914960/gobblin-7.png" alt="gobblin-7"/></p>

<ul>
<li>PartitionedDataWriter是整个过程的中心，它负责链接partitioner, PartitionAwareDataWriterBuilder,和writer.</li>
<li>partitioner负责计算每一个record的partition.</li>
<li>PartitionedDataWriter会在内存中创建LoadingCache<k,v>,存放partition和writer的哈希映射，PartitionAwareDataWriterBuilder负责为那些还没存放在内存的partition创建writer。</li>
<li>writer负责进行数据落地，即对同一partition的records调用其对应的writer写入数据。</li>
</ul>

<blockquote>
<p>注意 不管有没有需要partition, Gobblin的writer都是从PartitionedDataWriter开始,可以把不需要分区这种情况理解为只有一个分区。</p>
</blockquote>

<p>接下来让我们根据源码来展开整个过程吧</p>

<h2 id="toc_2">PartitionedDataWriter</h2>

<h4 id="toc_3">PartitionedDataWriter的初始化</h4>

<p>Writer流程得从上一篇文章<a href="http://lamborryan.github.io/gobblin-runtime-view/">《Gobblin系列四之Runtime初探》</a>上谈起。<br/>
Writer是在fork.class中processRecords方法通过buildWriterIfNotPresent创建的, 那么我们先从这里开始查看源码。</p>

<pre><code class="language-java">private void buildWriterIfNotPresent() throws IOException {
  if (!this.writer.isPresent()) {
    try {
      this.writer = Optional.of(this.closer.register(buildWriter()));
    } catch (SchemaConversionException sce) {
      throw new IOException(&quot;Failed to build writer for fork &quot; + this.index, sce);
    }
  }
}
/**
 * Build a {@link gobblin.writer.DataWriter} for writing fetched data records.
 */
private DataWriter&lt;Object&gt; buildWriter()
    throws IOException, SchemaConversionException {
  DataWriterBuilder&lt;Object, Object&gt; builder = this.taskContext.getDataWriterBuilder(this.branches, this.index)
      .writeTo(Destination.of(this.taskContext.getDestinationType(this.branches, this.index), this.taskState))
      .writeInFormat(this.taskContext.getWriterOutputFormat(this.branches, this.index))
      .withWriterId(this.taskId)
      .withSchema(this.convertedSchema.orNull())
      .withBranches(this.branches)
      .forBranch(this.index);

  return new PartitionedDataWriter&lt;&gt;(builder, this.taskContext.getTaskState());
}
</code></pre>

<p>从上述代码上看出, 一切的writer都开始于PartitionedDataWriter. 看看这个名字就可以看出gobblin默认就支持分区输出, 所以心里就放松了大半, 这个功能不需要花太大精力开发了。那么接下来就是分析PartitionedDataWriter这个class了。</p>

<h4 id="toc_4">PartitionedDataWriter的基类</h4>

<p>DataWriter做为interface定义了实现writer所需要实现的方法, 而PartitionedDataWriter继承了DataWriter,</p>

<pre><code class="language-java">public interface DataWriter&lt;D&gt; extends Closeable {

  /**
   * Write a source data record in Avro format using the given converter.
   *
   * @param record data record to write
   * @throws IOException if there is anything wrong writing the record
   */
  public void write(D record)
      throws IOException;

  /**
   * Commit the data written.
   *
   * @throws IOException if there is anything wrong committing the output
   */
  public void commit()
      throws IOException;

  /**
   * Cleanup context/resources.
   *
   * @throws IOException if there is anything wrong doing cleanup.
   */
  public void cleanup()
      throws IOException;

  /**
   * Get the number of records written.
   *
   * @return number of records written
   */
  public long recordsWritten();

  /**
   * Get the number of bytes written.
   *
   * @return number of bytes written
   */
  public long bytesWritten()
      throws IOException;
}
</code></pre>

<h4 id="toc_5">PartitionedDataWriter的具体实现</h4>

<p>PartitionedDataWriter他的代码还是比较简单的, 主要分为以下几个功能:</p>

<p>1.如果配置文件指定了writer.partitioner.class这个属性, 那么就创建这个partitioner的实例, 负责对record的分区。如果没有设置该属性,就意味着不需要进行分区,所以所有的record都分在一个partitioner中。</p>

<pre><code class="language-java">if (state.contains(ConfigurationKeys.WRITER_PARTITIONER_CLASS)) {
  // 如果设置了writer.partitioner.class属性则根据该类创建partitioner
  Preconditions.checkArgument(builder instanceof PartitionAwareDataWriterBuilder,
      String.format(&quot;%s was specified but the writer %s does not support partitioning.&quot;,
          ConfigurationKeys.WRITER_PARTITIONER_CLASS, builder.getClass().getCanonicalName()));

  try {
    this.shouldPartition = true;
    // 创建dataWriterBuilder，后续生成dataWriter
    this.builder = Optional.of(PartitionAwareDataWriterBuilder.class.cast(builder));
    // 创建partitionner
    this.partitioner = Optional.of(WriterPartitioner.class.cast(
        ConstructorUtils.invokeConstructor(Class.forName(state.getProp(ConfigurationKeys.WRITER_PARTITIONER_CLASS)),
            state, builder.getBranches(), builder.getBranch())));
  } catch (ReflectiveOperationException roe) {
    throw new IOException(roe);
  }
} else {
  // 如果没有设置了writer.partitioner.class, 则用NON_PARTITIONED_WRITER_KEY来表示只有一个partitioner
  this.shouldPartition = false;
  InstrumentedDataWriterDecorator&lt;D&gt; writer =
      this.closer.register(new InstrumentedDataWriterDecorator&lt;D&gt;(builder.build(), state));
  this.partitionWriters.put(NON_PARTITIONED_WRITER_KEY, writer);
  this.partitioner = Optional.absent();
  this.builder = Optional.absent();
}
</code></pre>

<p>所以需要在.pull配置writer.partitioner.class=gobblin.core.writer.partitioner.TimeBasedJsonWriterPartitioner来告诉gobblin按照TimeBasedJsonWriterPartitioner的规则进行partition</p>

<p>2.设置分区后, 在内存中以LoadingCache<k,v>存放record的partition和其对应的writer.</p>

<pre><code class="language-java">
private final LoadingCache&lt;GenericRecord, DataWriter&lt;D&gt;&gt; partitionWriters;

this.partitionWriters = CacheBuilder.newBuilder().build(new CacheLoader&lt;GenericRecord, DataWriter&lt;D&gt;&gt;() {
      @Override
      public DataWriter&lt;D&gt; load(final GenericRecord key) throws Exception {
        return closer
            .register(new InstrumentedPartitionedDataWriterDecorator&lt;D&gt;(createPartitionWriter(key), state, key));
      }
    });
</code></pre>

<blockquote>
<p>上述代码实现了回调, 当partitionWriters.get(key)时, 如果没有key,则通过load来创建新的writer</p>
</blockquote>

<p>3.为新的record partition创建writer。</p>

<pre><code class="language-java">private DataWriter&lt;D&gt; createPartitionWriter(GenericRecord partition) throws IOException {
  if (!this.builder.isPresent()) {
    throw new IOException(&quot;Writer builder not found. This is an error in the code.&quot;);
  }
  return this.builder.get().forPartition(partition).withWriterId(this.baseWriterId + &quot;_&quot; + this.writerIdSuffix++)
      .build();
}
</code></pre>

<p>writer的创建分为两步:</p>

<ul>
<li>根据partition, 创建PartitionAwareDataWriterBuilder, 即this.builder.get().forPartition(partition)返回partition对应的PartitionAwareDataWriterBuilder</li>
<li>调用PartitionAwareDataWriterBuilder的build创建DataWriter</li>
</ul>

<pre><code class="language-java">public abstract class PartitionAwareDataWriterBuilder&lt;S, D&gt; extends DataWriterBuilder&lt;S, D&gt; {

  protected Optional&lt;GenericRecord&gt; partition = Optional.absent();

  public PartitionAwareDataWriterBuilder&lt;S, D&gt; forPartition(GenericRecord partition) {
    this.partition = Optional.fromNullable(partition);
    return this;
  }

  public abstract boolean validatePartitionSchema(Schema partitionSchema);
}

</code></pre>

<p>关于PartitionAwareDataWriterBuilder的介绍请看文档<a href="https://github.com/linkedin/gobblin/wiki/Partitioned%20Writers">&lt;Partitioned Writers &gt;</a> ,结合它来看本文更容易理解。</p>

<p>例如以下的simpleWriterBuilder:</p>

<pre><code class="language-java">public class SimpleDataWriterBuilder extends FsDataWriterBuilder&lt;String, byte[]&gt; {
  @Override
  public DataWriter&lt;byte[]&gt; build() throws IOException {
    return new SimpleDataWriter(this, this.destination.getProperties());
  }
}
</code></pre>

<p>所以需要.pull文件中指定writer.builder.class=gobblin.writer.SimpleDataWriterBuilder来告诉gobblin我需要通过SimpleDataWriter来进行datawriter</p>

<p>4.writer 和 commit</p>

<pre><code class="language-java">@Override
public void write(D record) throws IOException {
  try {

    // 使用partitioner来计算record的partition
    GenericRecord partition =
        this.shouldPartition ? this.partitioner.get().partitionForRecord(record) : NON_PARTITIONED_WRITER_KEY;

    // 根据partition获取datawriter, 如果没有相应的datawriter则创建一个新的datawriter
    DataWriter&lt;D&gt; writer = this.partitionWriters.get(partition);

    // 调用datawriter的writer进行record的writer
    writer.write(record);
  } catch (ExecutionException ee) {
    throw new IOException(ee);
  }
}

@Override
public void commit() throws IOException {
  int writersCommitted = 0;
  for (Map.Entry&lt;GenericRecord, DataWriter&lt;D&gt;&gt; entry : this.partitionWriters.asMap().entrySet()) {
    try {
      // 对所有的datawriter调用commit
      entry.getValue().commit();
      writersCommitted++;
    } catch (Throwable throwable) {
      log.error(String.format(&quot;Failed to commit writer for partition %s.&quot;, entry.getKey()), throwable);
    }
  }
  if (writersCommitted &lt; this.partitionWriters.asMap().size()) {
    throw new IOException(&quot;Failed to commit all writers.&quot;);
  }
}
</code></pre>

<p>由此可见PartitionedDataWriter的writer和commit都是根据record的partition获取其对应的writer然后进行write和commit。 只不过在进行writer的时候如果还没该partition时会调用createPartitionWriter为该partition创建相应的writer。</p>

<p>至此关于PartitionedDataWriter的代码逻辑就介绍的差不多了。</p>

<h2 id="toc_6">partition</h2>

<p>所有partition.class都继承自接口WriterPartitioner, 他实现两个功能:</p>

<pre><code class="language-java">
public interface WriterPartitioner&lt;D&gt; {

  // partition的结构,
  public Schema partitionSchema();

  // 根据record计算partition
  public GenericRecord partitionForRecord(D record);

}
</code></pre>

<p>Gobblin默认支持时间为key的分区, 它实现了一个abstract class TimeBasedWriterPartitioner, 我们只需要根据自己的需要继承这个类做相应的开发就行了。比如我的record是json格式的, 那么我需要继承得到自己的实现类TimeBasedJsonWriterPartitioner.(Gobblin通过继承TimeBasedWriterPartitioner实现了TimeBasedAvroWriterPartitioner, 即对应record格式是avro的)</p>

<p>接下来让我们看看TimeBasedWriterPartitioner是怎么实现的以上两个方法的。</p>

<p>1.partitionForRecord方法</p>

<pre><code class="language-java">
@Override
 public GenericRecord partitionForRecord(D record) {
   // 虚函数，需要我们自己写解析方法, 即根据record的格式解析出里面的时间字段。
   long timestamp = getRecordTimestamp(record);

   // 按照partitionSchema方法产生的Schema封装timestamp为GenericRecord
   GenericRecord partition = new GenericData.Record(this.schema);
   if (!Strings.isNullOrEmpty(this.writerPartitionPrefix)) {
     partition.put(PREFIX, this.writerPartitionPrefix);
   }
   if (!Strings.isNullOrEmpty(this.writerPartitionSuffix)) {
     partition.put(SUFFIX, this.writerPartitionSuffix);
   }

   if (this.timestampToPathFormatter.isPresent()) {
     String partitionedPath = getPartitionedPath(timestamp);
     partition.put(PARTITIONED_PATH, partitionedPath);
   } else {
     DateTime dateTime = new DateTime(timestamp, this.timeZone);
     switch (this.granularity) {
       case MINUTE:
         partition.put(Granularity.MINUTE.toString(), dateTime.getMinuteOfHour());
       case HOUR:
         partition.put(Granularity.HOUR.toString(), dateTime.getHourOfDay());
       case DAY:
         partition.put(Granularity.DAY.toString(), dateTime.getDayOfMonth());
       case MONTH:
         partition.put(Granularity.MONTH.toString(), dateTime.getMonthOfYear());
       case YEAR:
         partition.put(Granularity.YEAR.toString(), dateTime.getYear());
     }
   }

   return partition;
 }

 // 虚函数，需要我们自己写解析方法, 即根据record的格式解析出里面的时间字段。
 public abstract long getRecordTimestamp(D record);
</code></pre>

<p>2.partitionSchema方法</p>

<pre><code class="language-java">
this.schema = getSchema();

private Schema getSchema() {
  if (this.timestampToPathFormatter.isPresent()) {
    return getDateTimeFormatBasedSchema();
  } else {
    return getGranularityBasedSchema();
  }
}

@Override
public Schema partitionSchema() {
  return this.schema;
}

</code></pre>

<p>该段代码的意思就是根据writer.partition.granularity这个配置项来组合Schema, 比如我设置了writer.partition.granularity=DAY.就返回Granularity.DAY.toString()</p>

<p>最后附上我的实现类TimeBasedJsonWriterPartitioner怎么实现getRecordTimestamp</p>

<pre><code class="language-java">
/**
 *  Check if the partition column value is present and is a Long object. Otherwise, use current system time.
 */
private long getRecordTimestamp(Optional&lt;Object&gt; writerPartitionColumnValue) {

    return writerPartitionColumnValue.orNull()  instanceof Long ? (Long) writerPartitionColumnValue.get()
            : System.currentTimeMillis();
}

@Override
public long getRecordTimestamp(byte[] record) {

    return getRecordTimestamp(getWriterPartitionColumnValue(record));
}


/**
 * Retrieve the value of the partition column field specified by this.partitionColumns
 */
private Optional&lt;Object&gt; getWriterPartitionColumnValue(byte[] record){
    if (!this.partitionColumns.isPresent()) {
        return Optional.absent();
    }

    Optional&lt;Object&gt; fieldValue = Optional.absent();

    for (String partitionColumn : this.partitionColumns.get()) {
        JSONObject jsonObject = new JSONObject(new String(record));
        fieldValue = Optional.of(jsonObject.get(partitionColumn));
        if(fieldValue.isPresent()){
            return fieldValue;
        }
    }

    return fieldValue;
}

</code></pre>

<p>上述代码实现了以下功能:</p>

<ul>
<li>根据配置writer.partition.columns获取字段名</li>
<li>解析json格式的record, 获取writer.partition.columns字段对应的时间戳</li>
<li>如果json解析失败则返回当前时间的时间戳。</li>
</ul>

<p>比如我设置了writer.partition.columns=timestamp</p>

<blockquote>
<p>我们需要writer.partition.timezone=Asia/Shanghai这个配置来指定时区,否则会出错。</p>
</blockquote>

<h2 id="toc_7">DataWriter</h2>

<p>Gobblin默认实现了SimpleDataWriter和AvroHDFSDataWriter, 它们都继承了FsDataWriter, FsDataWriter继承了DataWriter.</p>

<p>SimpleDataWriter将byte[]格式的record写入到文件系统(本地或者hdfs), 而AvroHDFSDataWriter则将avro格式的record写入到文件系统中.</p>

<p>以SimpleDataWriter为例, record被写入到stageFile目录中。</p>

<pre><code class="language-java">@Override
public void write(byte[] record) throws IOException {
  Preconditions.checkNotNull(record);

  byte[] toWrite = record;
  if (this.recordDelimiter.isPresent()) {
    toWrite = Arrays.copyOf(record, record.length + 1);
    toWrite[toWrite.length - 1] = this.recordDelimiter.get();
  }
  if (this.prependSize) {
    long recordSize = toWrite.length;
    ByteBuffer buf = ByteBuffer.allocate(Longs.BYTES);
    buf.putLong(recordSize);
    toWrite = ArrayUtils.addAll(buf.array(), toWrite);
  }
  this.stagingFileOutputStream.write(toWrite);
  this.bytesWritten += toWrite.length;
  this.recordsWritten++;
}

</code></pre>

<blockquote>
<p>每一个partition对应自己的dataWriter, 每一个dataWriter的作用范围也只能是自己的partition。</p>
</blockquote>

<h2 id="toc_8">Publiser</h2>

<p>到这一步为止, Gobblin已经将record按timestamp进行分区并按不同的目录写入到stageFile目录中。但是整个过程还没有完整。我们需要通过Publiser把数据publish到job－output上去。</p>

<p>刚好Gobblin已经帮我们实现了基于partition的publiser TimePartitionedDataPublisher.因此加入以下配置项即可:data.publisher.type=gobblin.publisher.TimePartitionedDataPublisher</p>

<h2 id="toc_9">总结</h2>

<p>本文简单介绍了整个writer的流程, 尤其是partition writer。 并结合源码具体介绍了 PartitionedDataWriter, PartitionAwareDataWriterBuilder, partition, datawriter, publiser这几个重要模块。</p>

<p>同时介绍了我自己实现TimeBasedJsonWriterPartitioner的过程。</p>

<p>最后是完整的partition writer的配置</p>

<pre><code class="language-bash">
kafka.brokers=x15:9091

source.class=gobblin.source.extractor.extract.kafka.KafkaSimpleSource
extract.namespace=gobblin.extract.kafka

topic.whitelist=biz_stats
writer.builder.class=gobblin.writer.SimpleDataWriterBuilder
simple.writer.delimiter=\n
simple.writer.prepend.size=false
writer.file.path.type=tablename
writer.destination.type=HDFS
writer.output.format=csv
writer.partitioner.class=xiaomei.gobblin.core.writer.partitioner.TimeBasedJsonWriterPartitioner
writer.partition.level=date
writer.partition.pattern=YYYY/MM/dd
writer.partition.columns=timestamp
writer.partition.timezone=Asia/Shanghai

data.publisher.type=gobblin.publisher.TimePartitionedDataPublisher

</code></pre>

<p>本文完</p>

<ul>
<li>原创文章，转载请注明： 转载自<a href="http://lamborryan.github.io">Lamborryan</a>，作者：<a href="http://lamborryan.github.io/about/">Ruan Chengfeng</a></li>
<li>本文链接地址：<a href="http://lamborryan.github.io/gobblin-state">http://lamborryan.github.io/gobblin-state</a></li>
<li>本文基于<a href="http://creativecommons.org/licenses/by/2.5/cn/">署名2.5中国大陆许可协议</a>发布，欢迎转载、演绎或用于商业目的，但是必须保留本文署名和文章链接。 如您有任何疑问或者授权方面的协商，请邮件联系我。</li>
</ul>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15155066323075.html">Gobblin系列(4)之Runtime初探</a></h1>
			<p class="meta"><time datetime="2018-01-09T22:03:52+08:00" 
			pubdate data-updated="true">2018/1/9</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<ul>
<li>
<a href="#toc_0">简介</a>
</li>
<li>
<a href="#toc_1">工作流</a>
<ul>
<li>
<ul>
<li>
<a href="#toc_2">commit/publiser 和 persist</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_3">任务流</a>
<ul>
<li>
<ul>
<li>
<a href="#toc_4">Task逻辑</a>
</li>
<li>
<a href="#toc_5">Fork 逻辑</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_6">总结</a>
</li>
</ul>


<h2 id="toc_0">简介</h2>

<p>Gobblin有两个重要的包即Gobblin-core和Gobblin-runtime, 前者实现了丰富的模块组件, 后者实现了完整的运行机制, 如此构成了Gobblin的高可扩展性的特点。</p>

<p>作为Gobblin的内核, Gobblin-runtime实现了Gobblin的工作流程和任务流程。本文名为Runtime初探, 主要研究目的就是通过Gobblin-runtime来了解Gobblin的运行逻辑, 便于后续对Gobblin的灵活开发。</p>

<p>在前文<a href="http://lamborryan.github.io/gobblin-first-exploration/">《Gobblin系列一之初探》</a>中提到了Gobblin具有工作流和任务流的概念。那么本文就分为工作流和任务流两块来介绍。</p>



		</div>

		 
		 <footer>
      	<a rel="full-article" href="15155066323075.html#more">read more &rarr;</a>
    	</footer>
    	

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15155065803565.html">Gobblin系列(3)之Azkaban Schedule</a></h1>
			<p class="meta"><time datetime="2018-01-09T22:03:00+08:00" 
			pubdate data-updated="true">2018/1/9</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<ul>
<li>
<a href="#toc_0">前言</a>
</li>
<li>
<a href="#toc_1">配置过程</a>
<ul>
<li>
<a href="#toc_2">注意事项</a>
</li>
<li>
<a href="#toc_3">安装Azkaban jobtype plugin</a>
</li>
<li>
<a href="#toc_4">Gobblin Job</a>
<ul>
<li>
<a href="#toc_5">创建Gobblin的azkaban Job</a>
</li>
<li>
<a href="#toc_6">创建gobblin job</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href="#toc_7">总结</a>
</li>
</ul>


<h2 id="toc_0">前言</h2>

<p>Gobblin支持三种Schedule即Quartz, Azkaban, Oozie, 默认是采用Quartz. 由于项目的工作流schedule已经采用Azkaban, 所以要将Gobblin Task配置到Azkaban. 但是没相到以为分分钟就能搞定的结果花了我整整一天的时间, 主要问题还是因为Gobblin的资料的匮乏, 在这实现过程中我查阅了Gobblin，Azkaban 和Azkaban-jobtype plugin的源码, 可见繁琐程度. 本文将描述怎么配置Gobblin-Azkaban，并结合主要的代码流程。之所以采用azkaban, 主要因为azkaban使用简单，能有效的进行工作流依赖管理。</p>



		</div>

		 
		 <footer>
      	<a rel="full-article" href="15155065803565.html#more">read more &rarr;</a>
    	</footer>
    	

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15155064360093.html">Gobblin系列(2)之History Store 和 Admin Server</a></h1>
			<p class="meta"><time datetime="2018-01-09T22:00:36+08:00" 
			pubdate data-updated="true">2018/1/9</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<ul>
<li>
<a href="#toc_0">简介</a>
</li>
<li>
<a href="#toc_1">History Store</a>
</li>
<li>
<a href="#toc_2">Admin Server</a>
<ul>
<li>
<a href="#toc_3">Web Ui</a>
</li>
<li>
<a href="#toc_4">client</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">总结</a>
<ul>
<li>
<a href="#toc_6">展示所有的job(相同的job只显示一个)</a>
</li>
<li>
<a href="#toc_7">展示job的所有历史记录</a>
</li>
<li>
<a href="#toc_8">展示某个job实例的detail信息, 可查看task, properties[View Job Properties]和metrix[View Metrics]</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">简介</h2>

<p>在阅读Gobblin的时候突然发现Gobblin其实是自带一个Admin Server以及History Store来存放历史的job运行数据, 所以也就研究了下这块内容。 这也反应了Gobblin的一个缺点即文档和资料的缺乏, 不过还好有源码。</p>



		</div>

		 
		 <footer>
      	<a rel="full-article" href="15155064360093.html#more">read more &rarr;</a>
    	</footer>
    	

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15155061559657.html">Gobblin系列(1)一之初探</a></h1>
			<p class="meta"><time datetime="2018-01-09T21:55:55+08:00" 
			pubdate data-updated="true">2018/1/9</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<ul>
<li>
<a href="#toc_0">简介</a>
</li>
<li>
<a href="#toc_1">框架</a>
</li>
<li>
<a href="#toc_2">工作流</a>
</li>
<li>
<a href="#toc_3">任务组成</a>
<ul>
<li>
<a href="#toc_4">Source and Extractor</a>
</li>
<li>
<a href="#toc_5">Converter</a>
</li>
<li>
<a href="#toc_6">Quality Checker</a>
</li>
<li>
<a href="#toc_7">Fork Operator</a>
</li>
<li>
<a href="#toc_8">Data Writer</a>
</li>
<li>
<a href="#toc_9">Data Publisher</a>
</li>
</ul>
</li>
<li>
<a href="#toc_10">任务流</a>
</li>
<li>
<a href="#toc_11">状态管理</a>
</li>
<li>
<a href="#toc_12">失败处理</a>
</li>
<li>
<a href="#toc_13">工作调度</a>
</li>
<li>
<a href="#toc_14">总结</a>
</li>
</ul>


<hr/>

<p>由于需要从kafka批量把日志dump到hdfs上，所以我们使用了linkin的Gobblin工具。Gobblin目前还处于开发阶段，资料比较少，文档介绍的也不是很详细，要使用他只能去阅读源码。因此我打算通过一系列的Gobblin的文章来记录下我的学习使用心得。</p>

<p>本文主要初步介绍Gobblin的基础框架, 主要内容来自<a href="https://github.com/linkedin/gobblin/wiki/Gobblin-Architecture">《官方文档》</a><br/>
, 然后在后续文章中展开介绍。</p>

<p>本人使用的Gobblin版本是0.6.2</p>



		</div>

		 
		 <footer>
      	<a rel="full-article" href="15155061559657.html#more">read more &rarr;</a>
    	</footer>
    	

	</article>
  
	<div class="pagination">
	
<a href="archives.html">Blog Archives</a>
	 
	    
	</div>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1.html"><strong>数仓建模&nbsp;(5)</strong></a>
	         <p class="cat-children-p"> 
	        
	        	<a href="StarSchema%E7%AC%94%E8%AE%B0.html">StarSchema笔记&nbsp;(4)</a>&nbsp;&nbsp;
	        
	        	<a href="Data%20Vault.html">Data Vault&nbsp;(1)</a>&nbsp;&nbsp;
	        
	         </p> 
	      </li>
	  
	      <li class="post">
	        <a href="%E5%BC%80%E6%BA%90%E6%8A%80%E6%9C%AF.html"><strong>开源技术&nbsp;(10)</strong></a>
	         <p class="cat-children-p"> 
	        
	        	<a href="Apache%20Calcite.html">Apache Calcite&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="Gobblin.html">Gobblin&nbsp;(8)</a>&nbsp;&nbsp;
	        
	        	<a href="Solr.html">Solr&nbsp;(1)</a>&nbsp;&nbsp;
	        
	         </p> 
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="15155074940397.html">Solr4.8.0源码分析汇总</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15155073098227.html">Gobblin系列(8)之Extractor源码分析</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15155071664887.html">Gobblin系列(7)之Source源码分析</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15155068855798.html">Gobblin系列(6)之State</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15155067914960.html">Gobblin系列(5)之Writer源码分析</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    




</body>
</html>